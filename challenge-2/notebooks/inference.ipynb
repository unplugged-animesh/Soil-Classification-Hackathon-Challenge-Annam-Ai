{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b24cef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, loader):\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device).unsqueeze(1)\n",
    "            outputs = model(images)\n",
    "            preds = (outputs > 0.5).float()\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    y_pred = np.array(all_preds).flatten()\n",
    "    y_true = np.array(all_labels).flatten()\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    print(f\"Validation Accuracy: {acc:.4f}\")\n",
    "    print(f\"Validation F1 Score: {f1:.4f}\")\n",
    "    print(\"Classification Report:\\n\", classification_report(y_true, y_pred))\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77338e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_threshold(model, loader):\n",
    "    model.eval()\n",
    "    all_probs, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device).unsqueeze(1)\n",
    "            outputs = model(images)\n",
    "            all_probs.extend(outputs.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    all_probs = np.array(all_probs).flatten()\n",
    "    all_labels = np.array(all_labels).flatten()\n",
    "    precisions, recalls, thresholds = precision_recall_curve(all_labels, all_probs)\n",
    "    f1_scores = 2 * (precisions * recalls) / (precisions + recalls + 1e-8)\n",
    "    best_index = np.argmax(f1_scores)\n",
    "    best_threshold = thresholds[best_index]\n",
    "    print(f\"Best threshold by F1: {best_threshold:.4f}, F1 Score: {f1_scores[best_index]:.4f}\")\n",
    "    return best_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51a9fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_threshold = find_best_threshold(model, val_loader)\n",
    "test_predictions = run_test_predictions(model, threshold=optimal_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65aa0550",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df = pd.DataFrame(test_predictions, columns=[\"image_id\", \"label\"])"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
